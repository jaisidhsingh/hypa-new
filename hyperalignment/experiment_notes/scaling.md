# Smaller TE (all-Mini-LM-L12-v2)

- Total = 12, encoder-batch-size = 4
- Num training epochs = 20
- FLOPs for 20 epochs = 288.166 Trillion
- `{'exp_name': 'vit_small_patch16_224_all-MiniLM-L12-v2', 'seed': 0, 'eval': {'epoch_1': {'cifar10': 86.1, 'cifar100': 36.26, 'imagenet1k': 12.41}, 'epoch_2': {'cifar10': 89.33, 'cifar100': 43.29, 'imagenet1k': 17.05}, 'epoch_5': {'cifar10': 89.48, 'cifar100': 46.8, 'imagenet1k': 19.63}, 'epoch_10': {'cifar10': 89.07, 'cifar100': 48.9, 'imagenet1k': 22.84}, 'epoch_20': {'cifar10': 88.62, 'cifar100': 49.27, 'imagenet1k': 23.21}}}`

# Bigger TE (all-roberta-large-v1)

- Total = 12, encoder-batch-size = 4
- Num training epochs = 20
- FLOPs for 20 epochs = 546.833 Trillion
