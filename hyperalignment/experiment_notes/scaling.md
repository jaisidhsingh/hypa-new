# Smaller TE (all-Mini-LM-L12-v2)

- Total = 12, encoder-batch-size = 4
- Num training epochs = 20
- FLOPs for 20 epochs = 288.166 Trillion

# Bigger TE (all-roberta-large-v1)

- Total = 12, encoder-batch-size = 4
- Num training epochs = 20
- FLOPs for 20 epochs = 546.833 Trillion
