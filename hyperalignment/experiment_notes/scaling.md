# Smaller TE

- Total = 12, encoder-batch-size = 4
- Num training epochs = 20
- FLOPs for 20 epochs = 288.166 Trillion

# Bigger TE


- Total = 12, encoder-batch-size = 4
- Num training epochs = 20
- FLOPs for 20 epochs = 288.166 Trillion