# Verifying effect of batch size in APE

## 1. `bs=256` & `lr=1e-3`

- Training dataset = cc595k
- FLOPs for 1 epoch = 936.169 B